{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Project Jupyter Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs for Optional Functions\n",
    "\n",
    "設定某些 Optional 功能\n",
    "\n",
    "- `bool PULL_FROM_REPO` : 是否要從 remote repo 把其他檔案 pull 下來\n",
    "- `bool DOWNLOAD_DATASET` : 是否要重新下載 dataset\n",
    "- `bool IMAGE_PREPORCESSING` : 是否要對原影像進行前處理\n",
    "- `int  MAX_NUM_CLASSES` : 最大 class 數量限制，若為 `None` 則不進行限制\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional configs\n",
    "\n",
    "PULL_FROM_REPO = False\n",
    "DOWNLOAD_DATASET = False\n",
    "IMAGE_PREPORCESSING = False\n",
    "MAX_NUM_CLASSES = None\n",
    "\n",
    "FINE_TUNE = False\n",
    "PRETRAINED_CLASSES = 500\n",
    "PRETRAINED_MODEL_PATH = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Project From ( Optional )\n",
    "\n",
    "如果是用 Colab 開啟此 GitHub 檔案的話，請將 PullFromRepo 設成 True，他會自動 Pull 對應 branch 的其他檔案下來（主要是 functional.py 以及 light_cnn.py 兩個檔案作為 dependency）。\n",
    "\n",
    "如果是使用 git clone 整個專案到 local 上的話，則不需要 PullFromRepo（預設為此）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if PULL_FROM_REPO:\n",
    "    !git init\n",
    "    !git remote add origin \"https://github.com/freshLiver/CV_PROJECT\"\n",
    "    !git pull origin res-9\n",
    "\n",
    "    %pip install gdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "下載資料集以及其他 training 所需的 packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from os import system\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from functional import TrainingHelper, ImageList\n",
    "from light_cnn import LightCNN_9Layers as LightCNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters\n",
    "\n",
    "訓練所需的參數以及資料集路徑設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "EPOCHS = 20\n",
    "BS = 128             # batch size\n",
    "LR = 0.001          # learning rate\n",
    "NUM_WORKER = 0\n",
    "PRINT_FREQUENCY = 10\n",
    "VALID_RATIO = 0.2\n",
    "\n",
    "\n",
    "# give me abs path as ROOT(work dir)\n",
    "ROOT = Path.home().joinpath(\"Downloads\")\n",
    "\n",
    "\n",
    "# list of data sources, assign one to DATA\n",
    "DATA_LIST = [\n",
    "    {\n",
    "        \"name\": \"vggface2-test.zip\",\n",
    "        \"id\": \"11zQKShQ_qTt5HJtZDpkvskPXuDb2rbbo\",\n",
    "        \"root\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"vggface2-test.zip\",\n",
    "        \"id\": \"11KFpKd8i8r1nES1AmSminorvRivB2M8_\",\n",
    "        \"root\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"pins-face-recognition.zip\",\n",
    "        \"id\": \"16dz9GQcMPkUILNypd4b4Ime7mlOEC-fN\",\n",
    "        \"root\": \"105_classes_pins_dataset\"\n",
    "    }\n",
    "]\n",
    "DATA = DATA_LIST[0]\n",
    "\n",
    "\n",
    "# download dataset from DATA_SRC to DATA_DST\n",
    "DATA_SRC = f'https://drive.google.com/uc?id={DATA[\"id\"]}'\n",
    "DATA_DST = ROOT.joinpath(DATA[\"name\"])\n",
    "\n",
    "\n",
    "# unzip dataset(DATA_DST) to UNZIP_DST, and all class dirs will under DATA_ROOT\n",
    "UNZIP_DST = ROOT.joinpath(\"dataset\")\n",
    "DATA_ROOT = UNZIP_DST.joinpath(DATA[\"root\"])            # real dataset dir\n",
    "\n",
    "\n",
    "# training and validation list file\n",
    "LABEL_DELIM = \" | \"\n",
    "TRAIN_LIST = ROOT.joinpath(\"train_list.txt\")\n",
    "VALID_LIST = ROOT.joinpath(\"valid_list.txt\")\n",
    "\n",
    "\n",
    "# logs(loss and acc img, model checkpoint) files\n",
    "LOG_DIR = ROOT.joinpath(\"logs\")\n",
    "if not LOG_DIR.exists():\n",
    "    LOG_DIR.mkdir()\n",
    "\n",
    "LOSS_IMG = LOG_DIR.joinpath(\"loss.png\")\n",
    "ACCURACY_IMG = LOG_DIR.joinpath(\"acc.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Unzip Dataset ( Optional )\n",
    "\n",
    "由於 vggface2 的 dataset 是放在我的 google drive 上，因此這邊會使用 gdown 下載資料集到前面設定的路徑（ROOT）下並解壓縮，若已經有下載 dataset 的話請在上面設定路徑，並不要執行這邊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_DATASET:\n",
    "\n",
    "    # download dataset\n",
    "    gdown.download(str(DATA_SRC), str(DATA_DST), False)\n",
    "\n",
    "    # extract if dir not exists\n",
    "    if not UNZIP_DST.exists():\n",
    "        system(f\"unzip {DATA_DST} -d {UNZIP_DST} > unzip.log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse, Split and Save Dataset\n",
    "\n",
    "讀取所有設定路徑下的圖片，並將各個 class 的圖片以及對應的 class 依據前面設定的比例分割成 training dataset 以及 validation dataset，並寫入到指定的檔案中（TRAIN_LIST 以及 VALID_LIST）。\n",
    "\n",
    "#### Image Trick (Optional)\n",
    "\n",
    "直接對原始圖片進行一些特別的處理，並複寫原圖片以在 Dataset 讀寫時減少運算量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into dict\n",
    "train_data = []\n",
    "valid_data = []\n",
    "\n",
    "NUM_CLASSES = 0\n",
    "CLASS_MAPPING = {}\n",
    "for index, subdir in enumerate(DATA_ROOT.glob(\"*\")):\n",
    "\n",
    "    # iterate each file in this dir\n",
    "    image_list = []\n",
    "    for imgPath in subdir.glob(\"*\"):\n",
    "\n",
    "        # push relative image path to image list\n",
    "        image_list.append((Path.relative_to(imgPath, ROOT), index))\n",
    "\n",
    "        if IMAGE_PREPORCESSING:\n",
    "\n",
    "            # load img\n",
    "            origin = cv2.imread(str(imgPath), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # do canny on original image\n",
    "            edges = cv2.Canny(origin, 150, 150)\n",
    "            res = cv2.bitwise_or(origin, edges)\n",
    "\n",
    "            # overwrite original image\n",
    "            cv2.imwrite(str(imgPath), res)\n",
    "\n",
    "    # split into train, validation list\n",
    "    tSize = math.floor(len(image_list)*(1-VALID_RATIO))\n",
    "\n",
    "    train_data += image_list[:tSize]\n",
    "    valid_data += image_list[tSize:]\n",
    "\n",
    "    # add label:class mapping\n",
    "    CLASS_MAPPING[NUM_CLASSES] = subdir.name\n",
    "\n",
    "    # num of finished classes\n",
    "    NUM_CLASSES += 1\n",
    "\n",
    "    # add NUM_CLASSES limit for faster testing\n",
    "    if MAX_NUM_CLASSES is not None and NUM_CLASSES >= MAX_NUM_CLASSES:\n",
    "        break\n",
    "\n",
    "\n",
    "# save to file\n",
    "with open(TRAIN_LIST, 'w') as f:\n",
    "    for img, cat in train_data:\n",
    "        f.write(f'{img}{LABEL_DELIM}{cat}\\n')\n",
    "\n",
    "with open(VALID_LIST, 'w') as f:\n",
    "    for img, cat in valid_data:\n",
    "        f.write(f'{img}{LABEL_DELIM}{cat}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FINE_TUNE:\n",
    "    with open(PRETRAINED_MODEL_PATH, \"rb\") as pretrained:\n",
    "        model = LightCNN(num_classes=PRETRAINED_CLASSES)\n",
    "\n",
    "        # load model state dict\n",
    "        if torch.cuda.is_available():\n",
    "            model.load_state_dict(torch.load(pretrained, map_location=\"cuda\"))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(pretrained, map_location=\"cpu\"))\n",
    "\n",
    "        # adjust new classes\n",
    "        model.fc2 = torch.nn.Linear(256, NUM_CLASSES)\n",
    "\n",
    "else:\n",
    "    model = LightCNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "設定 Training 以及 Validation 所需的 transforms 以及 data loader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.RandomCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.5], [.5])\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.5], [.5])\n",
    "])\n",
    "\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(\n",
    "    ImageList(root=ROOT, fileList=TRAIN_LIST, label_delim=LABEL_DELIM, transform=train_transform),\n",
    "    batch_size=BS,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKER,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    ImageList(root=ROOT, fileList=VALID_LIST, label_delim=LABEL_DELIM, transform=valid_transform),\n",
    "    batch_size=BS,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKER,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Visualize Result\n",
    "\n",
    "開始 Train 以及 Validate，並紀錄每個 Epoch 的 avg loss 以及 avg accuracy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "helper = TrainingHelper(\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=valid_loader,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BS,\n",
    "    learning_rate=LR,\n",
    "    print_frequency=PRINT_FREQUENCY,\n",
    "    model=model,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(params=model.parameters(), lr=LR)\n",
    ")\n",
    "\n",
    "for iEpoch in range(0, EPOCHS):\n",
    "\n",
    "    # train for one epoch\n",
    "    train_loss, train_acc = helper.train(iEpoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    valid_loss, valid_acc = helper.validate(iEpoch)\n",
    "\n",
    "    # log epoch result\n",
    "    helper.LOGGER.push(train_loss, train_acc, valid_loss, valid_acc)\n",
    "\n",
    "    # save model and logs in each epoch\n",
    "    helper.LOGGER.save(LOG_DIR.joinpath(f\"e{iEpoch}-log.json\"))\n",
    "    torch.save(model.state_dict(), LOG_DIR.joinpath(f\"e{iEpoch}-model.pth\"))\n",
    "\n",
    "\n",
    "# plot loss and accuracy\n",
    "helper.LOGGER.visualize(loss_dst=LOSS_IMG, accuracy_dst=ACCURACY_IMG)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
